{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import compose, ensemble, impute, linear_model, metrics, model_selection, naive_bayes, neighbors, pipeline, preprocessing, svm, tree\n",
    "from skopt import searchcv\n",
    "from scipy import stats\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wine-quality.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change spaces in column names to underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(' ', '_')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features: list[str] = df.select_dtypes(exclude=\"object\").columns.array\n",
    "categorial_features: list[str] = df.select_dtypes(include=\"object\").columns.array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "## Distribution of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16), tight_layout=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    sns.kdeplot(df[numeric_features[i]], fill=True, ax=ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These histograms tell me that the distribution of the data for all of the skewed to the left. This means that I will need to do some data transformation on the fields I decide to keep, so that the data is more normally distributed.\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(12, 1, figsize=(20, 16), tight_layout=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    sns.boxplot(x=df[numeric_features[i]], ax=ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These boxplots tell me that there are some outliers in the data. I will need to decide if I want to keep these outliers or not.\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the fields, I decided to do some trail and error to find out what percentile of the data (top and bottom percentile) I want to replace with the median value of the column, keeping the percentile value change to a minimum so as to not impute too many fields, but also to remove as many outliers as possible. Here are the outcomes of my trail and erroring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_config: dict[str, tuple[int, int]] = {\n",
    "    \"fixed_acidity\": (0.03, 0.92),\n",
    "    \"volatile_acidity\": (0, 0.81),\n",
    "    \"citric_acid\": (0.06, 0.95),\n",
    "    \"residual_sugar\": (0, 0.93),\n",
    "    \"chlorides\": (0, 0.92),\n",
    "    \"free_sulfur_dioxide\": (0, 0.9),\n",
    "    \"total_sulfur_dioxide\": (0, 0.95),\n",
    "    \"density\": (0, 0.99),\n",
    "    \"pH\": (0.01, 0.98),\n",
    "    \"sulphates\": (0, 0.96),\n",
    "    \"alcohol\": (0, 0.99),\n",
    "    \"quality\": (0.01, 0.97)\n",
    "}\n",
    "\n",
    "for feature, iqr_range in trim_config.items():\n",
    "    df[f\"{feature}_trimmed\"] = df[feature].clip(\n",
    "        df[feature].quantile(iqr_range[0]),\n",
    "        df[feature].quantile(iqr_range[1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(12, 1, figsize=(20, 24), tight_layout=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    feature = numeric_features[i]\n",
    "    sns.boxplot(\n",
    "        pd.melt(df[[feature, f\"{feature}_trimmed\"]]),\n",
    "        x=\"value\",\n",
    "        y=\"variable\",\n",
    "        ax=ax\n",
    "    ).set(\n",
    "        xlabel=None,\n",
    "        ylabel=None\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwriting the original columns with the trimmed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numeric_features:\n",
    "    df[feature] = df[f\"{feature}_trimmed\"]\n",
    "    df: pd.DataFrame = df.drop(columns=[f\"{feature}_trimmed\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    df[numeric_features].corr(),\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation heatmap tells me that the features have a moderate correlation with each other. This means that I will need to do some feature selection to reduce the number of features I use in my model.\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of each feature with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16), tight_layout=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    sns.lineplot(df, x=\"quality\", y=numeric_features[i], ax=ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs tell me that some of the features are more likely to affect the quality of the wine than others since the lines are more linear than the others, and can be used to predict the quality of the wine better. \n",
    "\n",
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Model Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normally Distributing the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to transform each column individually so that they aren't skewed to either direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    [df[feature].skew() for feature in numeric_features],\n",
    "    index=numeric_features,\n",
    "    columns=[\"Skew\"]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, figsize=(20, 12), tight_layout=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    sns.kdeplot(df[numeric_features[i]], fill=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numeric_features:\n",
    "    df[f\"{feature}_transformed\"] = stats.boxcox(df[feature])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    [[df[feature].skew(), df[f\"{feature}_transformed\"].skew()] for feature in numeric_features],\n",
    "    index=numeric_features,\n",
    "    columns=[\"Skew before transformation\", \"Skew after transformation\"]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 3, figsize=(20, 24), tight_layout=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # some magic to get the right feature name\n",
    "    feature = numeric_features[i - ((i % 6 > 2) + i // 6) * 3]\n",
    "    if i % 6 > 2:\n",
    "        sns.kdeplot(df[f\"{feature}_transformed\"], fill=True, color=\"green\", ax=ax)\n",
    "    else:\n",
    "        sns.kdeplot(df[feature], fill=True, ax=ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwriting the original columns with the transformed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numeric_features:\n",
    "    df[feature] = df[f\"{feature}_transformed\"]\n",
    "    df: pd.DataFrame = df.drop(columns=[f\"{feature}_transformed\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a pipeline with a standard scaler to normalize my numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipeline = pipeline.Pipeline([\n",
    "    (\"scaler\", preprocessing.MinMaxScaler())\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a pipeline with a one hot encoder to one hot encode the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_pipeline = pipeline.Pipeline([\n",
    "    (\"onehot\", preprocessing.OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying the target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to split the output into 3 classes, 1 for low quality, 1 for medium quality, and 1 for high quality. This is because my output variable is not exactly continuous (since the value is either 3, 4, 5, 6, 7, 8 or 9). However if I use regression to solve this, I will have to use a continuous output variable, so I decided to use classification instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"quality\"], bins=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above histogram we can roughly split the data into 3 classes, 1 for low quality, 1 for medium quality, and 1 for high quality. I will use this to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"quality_class\"] = np.where(df[\"quality\"] <= 4, \"low\", np.where(df[\"quality\"] >= 7, \"high\", \"medium\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"quality_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"quality\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Draft of Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to have at least one draft of the model before I can decide which features to keep, remove and scale for Feature Engineering. Becuase of this, I will run a simple RandomForestClassifier model with BayesSearchCV to find the best parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_bayes = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters here were found through a previous search, in the cases where I don't want to run bayes again since it can be very time consuming during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_optimal = {\n",
    "    \"max_depth\": 5697,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"min_samples_split\": 3,\n",
    "    \"n_estimators\": 665\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline = pipeline.Pipeline([\n",
    "    (\"preprocessor\", compose._column_transformer.ColumnTransformer([\n",
    "        (\"numeric\", numeric_pipeline, df[[feature for feature in numeric_features if feature != \"quality\"]].columns),\n",
    "        (\"categorical\", categorical_pipeline, df[categorial_features].columns)\n",
    "    ])),\n",
    "    (\"classifier\", ensemble.RandomForestClassifier(\n",
    "        **({} if running_bayes else bayes_optimal),\n",
    "        random_state=hash(\"2100326D\") % 2 ** 32,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"quality_class\"])\n",
    "y = df[\"quality_class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7, random_state=hash(\"2100326D\") % 2 ** 32)\n",
    "X_train: pd.DataFrame\n",
    "X_test: pd.DataFrame\n",
    "y_train: pd.Series\n",
    "y_test: pd.Series"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now depending on whether I've run BayesSearchCV, I will either load the best parameters found or calculate them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if running_bayes:\n",
    "    # Fix a bug in scikit-optimize\n",
    "    np.int = int\n",
    "\n",
    "    bs = searchcv.BayesSearchCV(\n",
    "        rfc_pipeline,\n",
    "        {\n",
    "            \"classifier__max_depth\": (1, 10000),\n",
    "            \"classifier__min_samples_leaf\": (1, 5),\n",
    "            \"classifier__min_samples_split\": (2, 5),\n",
    "            \"classifier__n_estimators\": (1, 1000)\n",
    "        },\n",
    "        n_iter=250,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    bs.fit(X_train, y_train)\n",
    "    bayes_optimal = {\n",
    "        \"max_depth\": bs.best_params_[\"classifier__max_depth\"],\n",
    "        \"min_samples_leaf\": bs.best_params_[\"classifier__min_samples_leaf\"],\n",
    "        \"min_samples_split\": bs.best_params_[\"classifier__min_samples_split\"],\n",
    "        \"n_estimators\": bs.best_params_[\"classifier__n_estimators\"]\n",
    "    }\n",
    "\n",
    "    print(\"Best params:\", bayes_optimal)\n",
    "    print(\"Best score:\", bs.best_score_)\n",
    "    print(\"Train score:\", bs.score(X_train, y_train))\n",
    "    print(\"Test score:\", bs.score(X_test, y_test))\n",
    "else:\n",
    "    rfc_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Train score:\", rfc_pipeline.score(X_train, y_train))\n",
    "    print(\"Test score:\", rfc_pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "rfc_pipeline: pipeline.Pipeline = bs.best_estimator_ if running_bayes else rfc_pipeline\n",
    "_transformer: compose._column_transformer.ColumnTransformer = rfc_pipeline.steps[0][1]\n",
    "_classifier: ensemble.RandomForestClassifier = rfc_pipeline.steps[1][1]\n",
    "_categorical_pipeline: pipeline.Pipeline = _transformer.transformers_[1][1]\n",
    "_onehot: preprocessing.OneHotEncoder = _categorical_pipeline.steps[0][1]\n",
    "\n",
    "importances = pd.DataFrame(\n",
    "    _classifier.feature_importances_,\n",
    "    index=list(X_train.drop(columns=[\"type\"]).columns) + _onehot.get_feature_names_out().tolist(),\n",
    "    columns=[\"Importance\"]\n",
    ").sort_values(by=\"Importance\", ascending=True).T\n",
    "\n",
    "sns.barplot(\n",
    "    data=importances.T[::-1],\n",
    "    x=\"Importance\",\n",
    "    y=importances.columns[::-1],\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I do feature selection, here is a reusable function to fit the model and return it's scores. It modifies the pipelines to work when some columns are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with_piped_features(piped_features: list[str], running_combinations = False) -> tuple[int, int]:\n",
    "    rfc_pipeline = pipeline.Pipeline([\n",
    "        (\"preprocessor\", compose._column_transformer.ColumnTransformer([\n",
    "            (\"numeric\", numeric_pipeline,\n",
    "                df[[feature for feature in piped_features if feature != \"type\"]].columns\n",
    "            ),\n",
    "            (\"categorical\", categorical_pipeline,\n",
    "                df[[\"type\"]].columns if \"type\" in piped_features else []\n",
    "            )\n",
    "        ])),\n",
    "        (\"classifier\", ensemble.RandomForestClassifier(\n",
    "            **({} if running_combinations else bayes_optimal),\n",
    "            random_state=hash(\"2100326D\") % 2 ** 32,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    rfc_pipeline.fit(X_train[piped_features], y_train)\n",
    "    train_score = rfc_pipeline.score(X_train[piped_features], y_train)\n",
    "    test_score = rfc_pipeline.score(X_test[piped_features], y_test)\n",
    "\n",
    "    return train_score, test_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First I decided to try to fit the model slowly removing each feature in the order of feature importance ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_features = ([\"type\"] + list(numeric_features[:-1]))\n",
    "\n",
    "for i in range(len(piped_features)):\n",
    "    features = piped_features[i:]\n",
    "    train_score, test_score = fit_with_piped_features(features)\n",
    "\n",
    "    print(f\"Train score {features}:\", train_score)\n",
    "    print(f\"Test score {features}:\", test_score)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest score was `0.814` which was ok but I wanted to know if there are any better scores with other combinations of features I put into the model\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to run fit the model with all possible combination of the feature list to find which combination gives me the best score for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_combinations = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are the optimal values found in the previous run of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations_optimal = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates']\n",
    "scores_optimal = (0, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now depending on whether I've run the combinations already, I will either load the previous data or calculate it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_features = ([\"type\"] + list(numeric_features[:-1]))\n",
    "\n",
    "if running_combinations:\n",
    "    for i in range(1, len(piped_features)):\n",
    "        for features in itertools.combinations(piped_features, i):\n",
    "            features = list(features)\n",
    "            train_score, test_score = fit_with_piped_features(features)\n",
    "\n",
    "            print(f\"Train score {features}:\", train_score)\n",
    "            print(f\"Test score {features}:\", test_score)\n",
    "\n",
    "            if test_score > scores_optimal[1]:\n",
    "                print(\"Highscore!\")\n",
    "                combinations_optimal = features\n",
    "                scores_optimal = (train_score, test_score)\n",
    "\n",
    "            print()\n",
    "    print()\n",
    "else:\n",
    "    scores_optimal = fit_with_piped_features(combinations_optimal, running_combinations)\n",
    "\n",
    "print(\"Highest train score:\", scores_optimal[0])\n",
    "print(\"Highest test score:\", scores_optimal[1])\n",
    "print(\"Included Features:\", combinations_optimal)\n",
    "print(\"Excluded Features:\", [feature for feature in ([\"type\"] + list(numeric_features[:-1])) if feature not in combinations_optimal])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's funny how I got the highest model score when the feature with the highest importance was left out. Although this isn't a good practice, I will follow what the output of the code tells me about what the optimal features\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [feature for feature in numeric_features if feature not in [\"chlorides\", \"alcohol\"]]\n",
    "del categorial_features\n",
    "\n",
    "X_train = X_train.drop(columns=[\"type\", \"chlorides\", \"alcohol\"])\n",
    "y_train = y_train.drop(columns=[\"type\", \"chlorides\", \"alcohol\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I am done with Feature Engineering, I can build the final version of the model. I will also run bayes again to find the best parameters for my model since my dataset changed a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_bayes = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters here were found through a previous search, in the cases where I don't want to run bayes again since it can be very time consuming during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_optimal = {\n",
    "    \"max_depth\": 6211,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"n_estimators\": 129\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline = pipeline.Pipeline([\n",
    "    (\"preprocessor\", compose._column_transformer.ColumnTransformer([\n",
    "        (\"numeric\", numeric_pipeline, df[[feature for feature in numeric_features if feature != \"quality\"]].columns),\n",
    "    ])),\n",
    "    (\"classifier\", ensemble.RandomForestClassifier(\n",
    "        **({} if running_bayes else bayes_optimal),\n",
    "        random_state=hash(\"2100326D\") % 2 ** 32,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now depending on whether I've run BayesSearchCV already, I will either load the best parameters found or calculate them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if running_bayes:\n",
    "    # Fix a bug in scikit-optimize\n",
    "    np.int = int\n",
    "\n",
    "    bs = searchcv.BayesSearchCV(\n",
    "        rfc_pipeline,\n",
    "        {\n",
    "            \"classifier__max_depth\": (1, 10000),\n",
    "            \"classifier__min_samples_leaf\": (1, 5),\n",
    "            \"classifier__min_samples_split\": (2, 5),\n",
    "            \"classifier__n_estimators\": (1, 1000)\n",
    "        },\n",
    "        n_iter=250,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    bs.fit(X_train, y_train)\n",
    "    bayes_optimal = {\n",
    "        \"max_depth\": bs.best_params_[\"classifier__max_depth\"],\n",
    "        \"min_samples_leaf\": bs.best_params_[\"classifier__min_samples_leaf\"],\n",
    "        \"min_samples_split\": bs.best_params_[\"classifier__min_samples_split\"],\n",
    "        \"n_estimators\": bs.best_params_[\"classifier__n_estimators\"]\n",
    "    }\n",
    "\n",
    "    print(\"Best params:\", bs.best_params_)\n",
    "    print(\"Best score:\", bs.best_score_)\n",
    "    print(\"Train score:\", bs.score(X_train, y_train))\n",
    "    print(\"Test score:\", bs.score(X_test, y_test))\n",
    "else:\n",
    "    rfc_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Train score:\", rfc_pipeline.score(X_train, y_train))\n",
    "    print(\"Test score:\", rfc_pipeline.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
